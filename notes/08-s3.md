# S3
Simple Storage Service (S3) is an object key-value storage in the cloud.

## Key basics
* Object-based - upload files
* Max is 5TB in size per file
* Unlimited storage 
* Files are stored in buckets (similar to folders)
* Universal namespace - names must be unique globally
* Up-time of 99.99% guarantee and durability 99.999999999 (11x9s)
* Lifecycle management, versioning and encryption are available
* Data is stored redundently accross multiple facilities and devices

## Core fundamentals of an object
* Key - name of the file
* Value - the data 
* Version ID
* Metadata - data about data
* Sub-resources - bucket-specific configuration

## Data consistency model 
* Read-after-write consistency for `PUT` of **new** objects (read right away)
* Eventual consistency for overwrite `PUT` and `DELETE` (time to propagate)

## Storage tiers/classes 
* S3 (*STANDARD*) - Up-time of 99.99% guarantee and durability 99.999999999 (11x9s). Data is stored redundently accross multiple facilities and devices (multiple locations)
* S3 IA (*STANDARD_IA*) - For *infrequently accessed* data. Fast access when needed. Lower fee but you are charged everytime you read
* S3 IA One-zone (*ONEZONE_IA*) - Stores in one single AZ, durability 11x9, but availability is 99.5%. Costs 20% less than regular S3 IA
* Reduced Redundancy Storage (*RRS*) - Durability and availability of 99.9% over a year. Used for data that can be recreated if lost (not for critical data)
* Glacier (*GLACIER*)- Used for archiving, very cheap. **Very** infrequently accessed data, takes 3 - 5 hours to restore from Glacier
* **NEW**: Intelligent tiering - For unknown or unpredictable access patterns. 2 tiers (frequent and infrequent access). Automatically moves data to most cost-effective tier based on how frequently each object is accessed. No fees for accessing data but a monthly fee for monitoring/automation

The classes/tiers are defined per file, not on a bucket level.

## Security 

### Access control
* By default, all buckets are **private**
* You can set-up access control to buckets by:
  * Bucket policies (bucket level)
  * Access control lists - ACLs (object level)
* You can configure access logs (log all requests made to an S3 bucket). The logs can be written to a different bucket

### Encryption
* In transit - SSL/TLS
* At rest - server side encryption: 
  * S3 managed keys - SSE-S3
  * KMS managed keys - SSE-KMS
  * Server side encryption with customer provided keys - SSE-C
* Client side encryption (DIY)

If the file is encrypted at upload time, a header (`amz-server-side-encryption: aws:kms`) is required. You can enforce encryption by adding a policy to deny all `PUT` requests that don't contain the encryption header.

## CORS
Cross-Origin Resource Sharing - enable an object in a certain bucket to access a different file in a different bucket (for instance a static website, js files in a bucket, css files in another one, etc). By default, CORS is not allowed.

Security access through Cloudfront: signed cookies, signed URLs and origin access identity.

## Performance optimization
If you're consistently receiving >100 `PUT/LIST/DELETE` or >300 `GET`, there are some best practices guidelines to optimize S3 performance.

Types of workload:
* GET-intensive workloads - use Cloudfront to reduce latency for your users
* Mixed requests type workloads (mix of `GET/PUT/DELETE/LIST`) - key names can impact performance for intensive workloads. S3 uses the key name to determine which partition an object will be stored in. Sequential key names (numbers, timestamps, alphabetical order) increase the likelihood to have files stored in the same partition and for heavy workloads this can cause I/O issues and contention. Use **random prefixes for key names** to force S3 to distribute your keys accross multiple partitions, distributing the I/O workload. **Important**: this is deprecated, doesn't apply anymore.


## Exam tips
* Remember it's object-based (**not OS and not DBs**)
* Max size per file is 5TB
* Files are stored in buckets
* Universal namespace (independent of region)
* Remember the data consistency model 
* Remember storage classes/tiers
* Remember the core fundamentals of an S3 object
* You have to allow ACLs in the bucket in order to set a file to public accessed, otherwise access is forbidden
* Remember the performance optimization guidelines
* Remember the difference between the encryption types
* Read the FAQ in AWS documentation
* File size limit through a single `PUT` operation is 5GB
* AWS recommends that you use Multipart-Upload for files larger than 100MB
